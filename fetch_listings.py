#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∞–ø–µ—Ä marktplaats: –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (aiohttp) + BeautifulSoup.
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –û–±—Ö–æ–¥–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–æ–≤–∞—Ä—ã < 3—á –≤ –ë–î.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python fetch_listings.py --socks5
  python fetch_listings.py --socks5 --db-path bot.db
  pip install aiohttp aiohttp-socks
"""
# –ó–∞–≥—Ä—É–∂–∞–µ–º .env –¥–æ –∏–º–ø–æ—Ä—Ç–∞ telegram_bot (—á—Ç–æ–±—ã BOT_TOKEN –∏ MP_TELEGRAM_CHAT_ID –±—ã–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
from pathlib import Path as _Path
_path = _Path(__file__).resolve().parent / ".env"
if _path.exists():
    try:
        from dotenv import load_dotenv
        load_dotenv(_path, override=True)
    except ImportError:
        import os
        for line in _path.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                k, _, v = line.partition("=")
                k, v = k.strip(), v.strip().strip('"').strip("'")
                if k and k not in os.environ:
                    os.environ[k] = v

import argparse
import asyncio
import json
import re
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from types import SimpleNamespace

import aiohttp
from bs4 import BeautifulSoup

# –ò–º–ø–æ—Ä—Ç –ë–î –∏ Telegram –∏–∑ telegram_bot
sys.path.insert(0, str(Path(__file__).resolve().parent))
from telegram_bot.database import init_db, get_conn, get_workers_on_shift
from telegram_bot.telegram_sender import send_listing_to_next_worker, send_round_summary
from telegram_bot.email_sender import try_send_listing_email
from telegram_bot.config import TELEGRAM_CHAT_ID, ENVIRONMENT

MARTKPLAATS_BASE_URL = "https://marktplaats.nl"
# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–µ (offeredSince:Altijd = –≤—Å–µ, sortBy=SORT_INDEX = –ø–æ –¥–∞—Ç–µ)
SORT_HASH = "#offeredSince:Altijd|sortBy:SORT_INDEX|sortOrder:DECREASING"

# Slug –¥–ª—è /l/ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç /cp/ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (cp -> l)
CP_TO_L_SLUG: dict[str, str] = {
    "auto-kopen": "auto-s",  # Auto's: /cp/91/auto-kopen/ -> /l/auto-s/
}

# –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π (id -> RU)
CAT_ID_TO_RU: dict[int, str] = {
    1: "–ê–Ω—Ç–∏–∫–≤–∞—Ä–∏–∞—Ç –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ",
    31: "–ê—É–¥–∏–æ, –¢–í –∏ —Ñ–æ—Ç–æ",
    91: "–ê–≤—Ç–æ–º–æ–±–∏–ª–∏",
    2600: "–ê–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏",
    48: "–ê–≤—Ç–æ –ø—Ä–æ—á–µ–µ",
    201: "–ö–Ω–∏–≥–∏",
    289: "–ö–∞—Ä–∞–≤–∞–Ω—ã –∏ –∫–µ–º–ø–∏–Ω–≥",
    1744: "CD –∏ DVD",
    322: "–ö–æ–º–ø—å—é—Ç–µ—Ä—ã –∏ –ü–û",
    378: "–ö–æ–Ω—Ç–∞–∫—Ç—ã –∏ —Å–æ–æ–±—â–µ–Ω–∏—è",
    1098: "–£—Å–ª—É–≥–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã",
    395: "–ñ–∏–≤–æ—Ç–Ω—ã–µ –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏",
    239: "–°–¥–µ–ª–∞–π —Å–∞–º –∏ —Ä–µ–º–æ–Ω—Ç",
    445: "–í–µ–ª–æ—Å–∏–ø–µ–¥—ã –∏ –º–æ–ø–µ–¥—ã",
    1099: "–•–æ–±–±–∏ –∏ –¥–æ—Å—É–≥",
    504: "–î–æ–º –∏ –∏–Ω—Ç–µ—Ä—å–µ—Ä",
    1032: "–î–æ–º–∞ –∏ –∫–æ–º–Ω–∞—Ç—ã",
    565: "–î–µ—Ç–∏ –∏ –º–∞–ª—ã—à–∏",
    621: "–û–¥–µ–∂–¥–∞ | –ñ–µ–Ω—Å–∫–∞—è",
    1776: "–û–¥–µ–∂–¥–∞ | –ú—É–∂—Å–∫–∞—è",
    678: "–ú–æ—Ç–æ—Ü–∏–∫–ª—ã",
    728: "–ú—É–∑—ã–∫–∞ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
    1784: "–ú–∞—Ä–∫–∏ –∏ –º–æ–Ω–µ—Ç—ã",
    1826: "–£–∫—Ä–∞—à–µ–Ω–∏—è, —Å—É–º–∫–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã",
    356: "–ò–≥—Ä–æ–≤—ã–µ –∏ –≤–∏–¥–µ–æ–∏–≥—Ä—ã",
    784: "–°–ø–æ—Ä—Ç –∏ —Ñ–∏—Ç–Ω–µ—Å",
    820: "–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è",
    1984: "–ë–∏–ª–µ—Ç—ã",
    1847: "–°–∞–¥ –∏ —Ç–µ—Ä—Ä–∞—Å–∞",
    167: "–í–∞–∫–∞–Ω—Å–∏–∏",
    856: "–û—Ç–¥—ã—Ö",
    895: "–ö–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ",
    976: "–í–æ–¥–Ω—ã–π —Å–ø–æ—Ä—Ç –∏ –ª–æ–¥–∫–∏",
    537: "–ë—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞",
    1085: "–¢–æ–≤–∞—Ä—ã –¥–ª—è –±–∏–∑–Ω–µ—Å–∞",
    428: "–†–∞–∑–Ω–æ–µ",
}
DATA_ELEM_ID = "__NEXT_DATA__"
DEFAULT_CATEGORY_URL = f"{MARTKPLAATS_BASE_URL}/l/boeken/p/1/"
MAX_AGE_HOURS = 3.0
CONCURRENT_FETCHES = 8  # –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–≤ —Ç–∏–∫ N –∫–∞—Ç–µ–≥–æ—Ä–∏–π + N —Ç–æ–≤–∞—Ä–æ–≤)


def load_proxy() -> str:
    """–ü–µ—Ä–≤—ã–π –ø—Ä–æ–∫—Å–∏ –∏–∑ proxies.json."""
    proxies = load_all_proxies()
    return proxies[0] if proxies else ""


def load_all_proxies() -> list[str]:
    """–í—Å–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ proxies.json."""
    for base in (Path.cwd(), Path(__file__).resolve().parent):
        path = base / "proxies.json"
        if path.is_file():
            try:
                data = json.loads(path.read_text(encoding="utf-8").strip())
                if isinstance(data, list) and data:
                    return [str(p).strip() for p in data if p]
            except (json.JSONDecodeError, OSError) as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
            break
    return []


class ProxyRotator:
    """–†–æ—Ç–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ 403."""
    def __init__(self, proxies: list[str], socks5: bool = False):
        self.proxies = proxies
        self.socks5 = socks5
        self.idx = 0
        self._lock = asyncio.Lock()

    def current(self) -> str | None:
        return self.proxies[self.idx % len(self.proxies)] if self.proxies else None

    async def next_proxy(self) -> str | None:
        async with self._lock:
            self.idx += 1
            return self.current()

    def connector(self) -> aiohttp.BaseConnector:
        return _make_connector(self.current(), self.socks5)


def parse_proxy(proxy_str: str) -> tuple[str, int, str, str]:
    """(host, port, user, pass)."""
    from urllib.parse import urlparse

    s = proxy_str.strip()
    if s.startswith(("http://", "https://", "socks")):
        p = urlparse(s)
        return (
            p.hostname or "",
            int(p.port or 80),
            p.username or "",
            p.password or "",
        )
    parts = s.split(":", 3)
    if len(parts) == 4:
        return parts[0], int(parts[1]), parts[2], parts[3]
    if len(parts) >= 2:
        return parts[0], int(parts[1]), "", ""
    return s, 80, "", ""


def proxy_url(proxy_str: str, socks5: bool = False) -> str:
    """URL –¥–ª—è aiohttp/requests proxy."""
    host, port, user, pass_ = parse_proxy(proxy_str)
    scheme = "socks5" if socks5 else "http"
    if proxy_str.strip().startswith("socks"):
        return proxy_str.strip()
    if user and pass_:
        return f"{scheme}://{user}:{pass_}@{host}:{port}"
    return f"{scheme}://{host}:{port}"


def _make_connector(proxy: str | None, socks5: bool):
    """Connector –¥–ª—è aiohttp (—Å SOCKS5 –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)."""
    import aiohttp
    if not proxy:
        return aiohttp.TCPConnector()
    try:
        from aiohttp_socks import ProxyConnector
        px = proxy_url(proxy, socks5=socks5)
        return ProxyConnector.from_url(px)
    except ImportError:
        return aiohttp.TCPConnector()


HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "nl-NL,nl;q=0.9,en;q=0.8",
}


async def fetch_page_async(
    session: aiohttp.ClientSession,
    url: str,
    timeout: int = 30,
) -> str:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å HTML –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
    timeout_obj = aiohttp.ClientTimeout(total=timeout)
    async with session.get(url, headers=HEADERS, timeout=timeout_obj) as resp:
        resp.raise_for_status()
        return await resp.text()


def _is_403(e: Exception) -> bool:
    if isinstance(e, aiohttp.ClientResponseError):
        return e.status == 403
    return "403" in str(e) or "Forbidden" in str(e)


async def fetch_with_retry(
    url: str,
    rotator: ProxyRotator,
    timeout: int,
    max_retries: int = 5,
) -> str:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ 403 (—Å–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏)."""
    last_err: Exception | None = None
    for _ in range(max_retries):
        connector = rotator.connector()
        try:
            async with aiohttp.ClientSession(connector=connector) as session:
                html = await fetch_page_async(session, url, timeout)
                return html
        except Exception as e:
            last_err = e
            if _is_403(e) and rotator.proxies and len(rotator.proxies) > 1:
                await rotator.next_proxy()
                host, port = parse_proxy(rotator.current() or "")[:2]
                print(f"      ‚ö† 403 ‚Üí —Å–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏ –Ω–∞ {host}:{port}")
            else:
                raise
    raise last_err or RuntimeError("max retries")


def extract_next_data(html: str):
    """–ò–∑–≤–ª–µ—á—å __NEXT_DATA__ –∏–∑ HTML (BeautifulSoup)."""
    soup = BeautifulSoup(html, "lxml")
    script = soup.find("script", attrs={"id": DATA_ELEM_ID})
    if script and script.string:
        return json.loads(script.string)
    match = re.search(r'<script[^>]+id="__NEXT_DATA__"[^>]*>([^<]+)</script>', html)
    if match:
        return json.loads(match.group(1))
    return None


def parse_listings_from_next_data(data: dict) -> list[dict]:
    """–°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ __NEXT_DATA__."""
    props = data.get("props", {}).get("pageProps", {})
    sr = props.get("searchRequestAndResponse", {})
    return sr.get("listings", [])


def get_categories(html: str) -> list[tuple[int, str, str]]:
    """
    –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ HTML (hz-CategoryMenuBar-list), –∏–Ω–∞—á–µ –∏–∑ __CONFIG__.categoryLinks.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [(id, name, listing_url), ...]. URL —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ –Ω–æ–≤–∏–∑–Ω–µ.
    """
    result = _get_categories_from_html(html)
    if result:
        return result
    return _get_categories_from_config(html)


def _get_categories_from_html(html: str) -> list[tuple[int, str, str]]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ hz-CategoryMenuBar-list (–≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –º–µ–Ω—é)."""
    soup = BeautifulSoup(html, "lxml")
    ul = soup.find("ul", class_="hz-CategoryMenuBar-list")
    if not ul:
        return []
    result = []
    for a in ul.find_all("a", class_="hz-CategoryMenuBarItem-link"):
        href = a.get("href", "")
        if not href.startswith("/cp/"):
            continue
        parts = href.rstrip("/").split("/")
        if len(parts) < 4:
            continue
        try:
            cat_id = int(parts[2])
        except (ValueError, IndexError):
            continue
        slug = parts[3]
        l_slug = CP_TO_L_SLUG.get(slug, slug)
        name = a.get_text(strip=True) or slug
        listing_url = f"{MARTKPLAATS_BASE_URL}/l/{l_slug}/p/1/{SORT_HASH}"
        result.append((cat_id, name, listing_url))
    return result


def _get_categories_from_config(html: str) -> list[tuple[int, str, str]]:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ __CONFIG__.categoryLinks (fallback)."""
    config = extract_config(html)
    if not config:
        return []
    links = config.get("categoryLinks", [])
    result = []
    for link in links:
        url_path = link.get("url", "")
        if not url_path.startswith("/cp/"):
            continue
        parts = url_path.rstrip("/").split("/")
        slug = parts[-1] if len(parts) >= 4 else ""
        if not slug:
            continue
        cat_id = int(link.get("id", 0))
        name = link.get("name", slug)
        l_slug = CP_TO_L_SLUG.get(slug, slug)
        listing_url = f"{MARTKPLAATS_BASE_URL}/l/{l_slug}/p/1/{SORT_HASH}"
        result.append((cat_id, name, listing_url))
    return result


def details_to_db_row(details: dict, listing_url: str, parent_category_id: int) -> dict:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è details –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –ë–î."""
    image_urls = details.get("image_urls") or []
    img_str = "|".join(str(u) for u in image_urls) if image_urls else ""
    attrs = details.get("attributes") or []
    attrs_json = json.dumps(attrs, ensure_ascii=False) if attrs else ""

    return {
        "item_id": details.get("item_id", ""),
        "seller_id": details.get("seller_id", "") or None,
        "parent_category_id": parent_category_id,
        "child_category_id": details.get("category_id"),
        "category_verticals": details.get("category_name", "") or None,
        "ad_type": details.get("ad_type", "") or None,
        "title": details.get("title", "") or None,
        "description": details.get("description", "") or None,
        "price_type": details.get("price_type", "") or None,
        "price_cents": int(details.get("price_cents", 0) or 0),
        "types": None,
        "services": None,
        "listing_url": listing_url or None,
        "image_urls": img_str or None,
        "city_name": details.get("city_name", "") or None,
        "country_code": details.get("country_code", "") or None,
        "listed_timestamp": details.get("listed_timestamp", "") or None,
        "crawled_timestamp": datetime.now(timezone.utc).isoformat(),
        "view_count": int(details.get("view_count", 0) or 0),
        "favorited_count": int(details.get("favorited_count", 0) or 0),
        "seller_name": details.get("seller_name", "") or None,
        "latitude": None,
        "longitude": None,
        "distance_meters": None,
        "country_name": details.get("country_name", "") or None,
        "priority_product": None,
        "traits": None,
        "category_specific_description": None,
        "reserved": 1 if details.get("reserved") else 0,
        "nap_available": 0,
        "urgency_feature_active": 0,
        "is_verified": 1 if details.get("seller_verified") else 0,
        "seller_website_url": None,
        "attributes_json": attrs_json or None,
    }


def save_listing_to_db(row: dict, db_path: str) -> None:
    """–í—Å—Ç–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –≤ –ë–î."""
    init_db(db_path)
    conn = get_conn(db_path)
    cols = list(row.keys())
    placeholders = ", ".join("?" * len(cols))
    cols_str = ", ".join(cols)
    conn.execute(
        f"INSERT OR REPLACE INTO listings ({cols_str}) VALUES ({placeholders})",
        [row.get(c) for c in cols],
    )
    conn.commit()
    conn.close()


def load_existing_item_ids(db_path: str) -> set[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å item_id –∏–∑ –ë–î."""
    if not Path(db_path).exists():
        return set()
    init_db(db_path)
    conn = get_conn(db_path)
    rows = conn.execute("SELECT item_id FROM listings").fetchall()
    conn.close()
    return {str(r[0]) for r in rows if r[0]}


def age_hours(since: str | None) -> float | None:
    """–ß–∞—Å—ã —Å –º–æ–º–µ–Ω—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏. None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å."""
    if not since:
        return None
    try:
        dt = datetime.fromisoformat(since.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        delta = datetime.now(timezone.utc) - dt
        return delta.total_seconds() / 3600
    except (ValueError, TypeError):
        return None


def extract_config(html: str) -> dict | None:
    """–ò–∑–≤–ª–µ—á—å window.__CONFIG__ –∏–∑ HTML (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–≤–∞—Ä–∞)."""
    idx = html.find("window.__CONFIG__")
    if idx == -1:
        return None
    start = html.find("=", idx) + 1
    if start <= 0:
        return None
    # –ò—â–µ–º –Ω–∞—á–∞–ª–æ JSON –∏ –ø–∞—Ä—Å–∏–º —Å —É—á—ë—Ç–æ–º –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–∫–æ–±–æ–∫
    depth = 0
    json_start = -1
    for i, c in enumerate(html[start:], start):
        if c == "{":
            if depth == 0:
                json_start = i
            depth += 1
        elif c == "}":
            depth -= 1
            if depth == 0:
                try:
                    return json.loads(html[json_start : i + 1])
                except json.JSONDecodeError:
                    pass
                break
    return None


def parse_listing_details(config: dict) -> dict | None:
    """–í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–∞ –∏–∑ __CONFIG__.listing."""
    listing = config.get("listing")
    if not listing:
        return None
    stats = listing.get("stats", {})
    price_info = listing.get("priceInfo", {})
    # –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ç–æ–≤–∞—Ä–∞: seller (–Ω–µ sellerInformation), seller.location
    seller = listing.get("seller") or listing.get("sellerInformation") or listing.get("sellerInfo") or {}
    seller_loc = seller.get("location") or {}
    # category –∫–∞–∫ –æ–±—ä–µ–∫—Ç {id, name, fullName}
    category = listing.get("category") or {}

    # –ö–∞—Ä—Ç–∏–Ω–∫–∏: gallery.imageUrls –∏–ª–∏ gallery.media.images[].base
    image_urls = []
    gallery = listing.get("gallery") or {}
    urls = gallery.get("imageUrls") or []
    for u in urls:
        if isinstance(u, str) and u:
            image_urls.append(u if u.startswith("http") else "https:" + u)
    if not image_urls:
        for img in gallery.get("media", {}).get("images", []) or []:
            base = img.get("base", "")
            if base:
                image_urls.append(base if base.startswith("http") else "https:" + base)
    if not image_urls:
        for p in listing.get("pictures") or listing.get("images") or []:
            if isinstance(p, dict):
                u = p.get("extraExtraLargeUrl") or p.get("largeUrl") or p.get("url")
                if u:
                    image_urls.append(u)

    return {
        "item_id": listing.get("itemId", ""),
        "title": listing.get("title", ""),
        "description": (listing.get("description") or "")[:10000],
        "ad_type": listing.get("adType", ""),
        "price_type": price_info.get("priceType", ""),
        "price_cents": price_info.get("priceCents", 0),
        "listed_timestamp": stats.get("since"),
        "view_count": stats.get("viewCount", 0),
        "favorited_count": stats.get("favoritedCount", 0),
        "seller_id": str(seller.get("id") or seller.get("sellerId", "")),
        "seller_name": seller.get("name") or seller.get("sellerName", ""),
        "seller_verified": seller.get("isVerified", False),
        "seller_member_since": seller.get("activeSinceDiff") or seller.get("memberSince", ""),
        "city_name": seller_loc.get("cityName", ""),
        "country_code": seller_loc.get("countryAbbreviation", ""),
        "country_name": seller_loc.get("countryName", ""),
        "image_count": len(image_urls),
        "image_urls": image_urls,
        "category_id": category.get("id") or listing.get("categoryId"),
        "category_name": category.get("name") or category.get("fullName") or listing.get("categoryName", ""),
        "reserved": listing.get("reserved", False),
        "attributes": listing.get("attributes", []),
    }


async def _fetch_detail_with_retry(
    item: dict,
    rotator: ProxyRotator,
    timeout: int,
) -> tuple[dict | None, str, str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ 403."""
    vip_url = item.get("vipUrl")
    if not vip_url:
        return None, "", "no vipUrl"
    listing_url = f"{MARTKPLAATS_BASE_URL}{vip_url}"
    try:
        html = await fetch_with_retry(listing_url, rotator, timeout)
    except Exception as e:
        return None, listing_url, str(e)
    config = extract_config(html)
    if not config:
        return None, listing_url, "no config"
    details = parse_listing_details(config)
    if not details:
        return None, listing_url, "no details"
    return details, listing_url, ""


async def _worker_category(
    queue: asyncio.Queue,
    rotator: ProxyRotator,
    sem: asyncio.Semaphore,
    timeout: int,
    max_age_hours: float,
    remaining_limit: list[int],
    existing_item_ids: set[str],
    db_path: str,
    results: dict,
) -> None:
    """
    –í–æ—Ä–∫–µ—Ä: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏.
    concurrent=N: N –≤–æ—Ä–∫–µ—Ä–æ–≤, –≤ —Ç–∏–∫ –¥–æ N –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π + –¥–æ N –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤.
    """
    while True:
        try:
            task = queue.get_nowait()
        except asyncio.QueueEmpty:
            break
        idx, cat_id, cat_name, cat_url = task
        if remaining_limit and remaining_limit[0] <= 0:
            queue.put_nowait(task)
            break
        cat_display = CAT_ID_TO_RU.get(cat_id, cat_name)
        t0 = time.perf_counter()
        saved = 0
        try:
            async with sem:
                html = await fetch_with_retry(cat_url, rotator, timeout)
            data = extract_next_data(html)
            if not data:
                results[idx] = (0, time.perf_counter() - t0, False)
                continue
            all_listings = parse_listings_from_next_data(data)
            if not all_listings:
                results[idx] = (0, time.perf_counter() - t0, False)
                continue

            to_fetch: list[dict] = []
            limit_val = remaining_limit[0] if remaining_limit else 0
            for item in all_listings:
                if limit_val > 0 and saved + len(to_fetch) >= limit_val:
                    break
                if item.get("itemId", "") in existing_item_ids:
                    continue
                if not item.get("vipUrl"):
                    continue
                to_fetch.append(item)

            stale = False
            for item in to_fetch:
                if remaining_limit and remaining_limit[0] <= 0:
                    break
                async with sem:
                    details, listing_url, err = await _fetch_detail_with_retry(item, rotator, timeout)
                if details is None:
                    if err and "403" not in str(err):
                        title = item.get("title", "")[:50]
                        print(f"      ‚ö† [{cat_display}] ¬´{title}¬ª ‚Äî {err}")
                    continue
                title = details.get("title", "")[:50]
                hours = age_hours(details.get("listed_timestamp"))
                if hours is not None and hours >= max_age_hours:
                    print(f"      ‚Üí [{cat_display}] ¬´{title}¬ª ‚Äî {hours:.1f} —á (>3—á)")
                    stale = True
                    break
                item_id = details.get("item_id", "")
                row = details_to_db_row(details, listing_url, cat_id)
                save_listing_to_db(row, db_path)
                existing_item_ids.add(item_id)
                saved += 1
                if remaining_limit and remaining_limit[0] > 0:
                    remaining_limit[0] -= 1
                row["category_ru"] = CAT_ID_TO_RU.get(cat_id, "")
                ns = SimpleNamespace(**row)
                ok, worker_id = send_listing_to_next_worker(ns, db_path)
                print(f"      üì§ Telegram: {'OK' if ok else '–ø—Ä–æ–ø—É—Å–∫ (–Ω–µ—Ç –≤–æ—Ä–∫–µ—Ä–æ–≤ –Ω–∞ —Å–º–µ–Ω–µ)'} ¬´{title[:40]}¬ª", flush=True)
                try:
                    email_ok, recipient = try_send_listing_email(db_path, ns, worker_id)
                    if email_ok and recipient:
                        print(f"      üìß Email: –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ {recipient} ¬´{title[:40]}¬ª", flush=True)
                    elif email_ok:
                        print(f"      üìß Email: OK ¬´{title[:40]}¬ª", flush=True)
                    elif ENVIRONMENT == "dev":
                        print(f"      üìß Email: –ø—Ä–æ–ø—É—Å–∫ (–Ω–µ—Ç —à–∞–±–ª–æ–Ω–∞/–ø–æ—á—Ç) ¬´{title[:40]}¬ª", flush=True)
                except Exception as ex:
                    print(f"      ‚ö† Email: {ex}", flush=True)
                if hours is not None:
                    print(f"      ‚úì [{cat_display}] ¬´{title}¬ª ‚Äî {hours:.1f} —á")
                else:
                    print(f"      ‚úì [{cat_display}] ¬´{title}¬ª")

            results[idx] = (saved, time.perf_counter() - t0, stale)
        except Exception as e:
            print(f"      ‚ùå [{cat_display}] {e}")
            results[idx] = (0, time.perf_counter() - t0, False)


async def run_one_round_async(
    proxies: list[str],
    socks5: bool,
    timeout: int,
    max_age_hours: float,
    limit: int,
    db_path: str,
    existing_item_ids: set[str],
    concurrent: int = CONCURRENT_FETCHES,
) -> tuple[int, float, list[float]]:
    """
    –û–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥: concurrent –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
    –í —Ç–∏–∫: –¥–æ N –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π + –¥–æ N –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤ (–ø–æ 1 –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —Ç–æ–≤–∞—Ä –∏–∑ –∫–∞–∂–¥–æ–π).
    """
    t_round_start = time.perf_counter()
    rotator = ProxyRotator(proxies, socks5)
    sem = asyncio.Semaphore(concurrent)

    try:
        t0 = time.perf_counter()
        main_html = await fetch_with_retry(MARTKPLAATS_BASE_URL + "/", rotator, timeout)
        print(f"–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {time.perf_counter() - t0:.1f} —Å")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–∞–≤–Ω–æ–π: {e}")
        return 0, time.perf_counter() - t_round_start, []

    categories = get_categories(main_html)
    if not categories:
        print("‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return 0, time.perf_counter() - t_round_start, []

    queue: asyncio.Queue = asyncio.Queue()
    for idx, (cat_id, cat_name, cat_url) in enumerate(categories):
        queue.put_nowait((idx + 1, cat_id, cat_name, cat_url))

    results: dict[int, tuple[int, float, bool]] = {}
    remaining_limit = [limit] if limit > 0 else []
    workers = [
        asyncio.create_task(_worker_category(
            queue, rotator, sem, timeout, max_age_hours, remaining_limit,
            existing_item_ids, db_path, results,
        ))
        for _ in range(min(concurrent, len(categories)))
    ]
    await asyncio.gather(*workers)

    total_saved = 0
    category_times: list[float] = []
    for idx in range(1, len(categories) + 1):
        if idx in results:
            saved, cat_sec, stale = results[idx]
            total_saved += saved
            category_times.append(cat_sec)
            cat_display = CAT_ID_TO_RU.get(categories[idx - 1][0], categories[idx - 1][1])
            print(f"[{idx}/{len(categories)}] {cat_display} ‚Äî {saved} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, ‚è± {cat_sec:.1f} —Å" + (" (—É—Å—Ç–∞—Ä–µ–ª–∞)" if stale else ""))

    total_sec = time.perf_counter() - t_round_start
    return total_saved, total_sec, category_times


def main() -> int:
    parser = argparse.ArgumentParser(description="–°–∫—Ä–∞–ø–µ—Ä marktplaats: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–æ–≤–∞—Ä—ã < 3—á, –ë–î, –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª")
    parser.add_argument("--socks5", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SOCKS5")
    parser.add_argument("--no-proxy", action="store_true", help="–ë–µ–∑ –ø—Ä–æ–∫—Å–∏")
    parser.add_argument("--db-path", default="bot.db", help="–ü—É—Ç—å –∫ SQLite –ë–î")
    parser.add_argument("--timeout", type=int, default=30)
    parser.add_argument("--limit", type=int, default=0, help="–ú–∞–∫—Å. —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ —Ä–∞—É–Ω–¥ (0 = –±–µ–∑ –ª–∏–º–∏—Ç–∞)")
    parser.add_argument("--max-age", type=float, default=MAX_AGE_HOURS, help="–ú–∞–∫—Å. –≤–æ–∑—Ä–∞—Å—Ç –≤ —á–∞—Å–∞—Ö")
    parser.add_argument("--pause-minutes", type=int, default=10, help="–ü–∞—É–∑–∞ –≤ –º–∏–Ω. –∫–æ–≥–¥–∞ —Ä–∞—É–Ω–¥ –±–µ–∑ –Ω–æ–≤—ã—Ö")
    parser.add_argument("--once", action="store_true", help="–û–¥–∏–Ω —Ä–∞—É–Ω–¥ –∏ –≤—ã—Ö–æ–¥ (–±–µ–∑ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞)")
    parser.add_argument("--concurrent", type=int, default=CONCURRENT_FETCHES,
                        help="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–≤ —Ç–∏–∫: N –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π + N –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤)")
    args = parser.parse_args()

    if args.socks5:
        try:
            import aiohttp_socks  # noqa: F401
        except ImportError:
            print("–î–ª—è --socks5: pip install aiohttp-socks")
            return 1

    proxies: list[str] = []
    if not args.no_proxy:
        proxies = load_all_proxies()
        if not proxies:
            print("proxies.json –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç")
            return 1
    else:
        proxies = [""]  # –±–µ–∑ –ø—Ä–æ–∫—Å–∏ ‚Äî –æ–¥–∏–Ω "–ø—É—Å—Ç–æ–π" –ø—Ä–æ–∫—Å–∏
    print(f"–ü—Ä–æ–∫—Å–∏: {'–Ω–µ—Ç' if args.no_proxy else f'{len(proxies)} —à—Ç.'} ({'SOCKS5' if args.socks5 else 'HTTP'})")
    print(f"–ë–î: {args.db_path}")
    print(f"–ú–∞–∫—Å. –≤–æ–∑—Ä–∞—Å—Ç: {args.max_age} —á")
    print(f"–ü–∞—É–∑–∞: {args.pause_minutes} –º–∏–Ω.")
    print(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: {args.concurrent} –∑–∞–ø—Ä–æ—Å–æ–≤")
    _tok = "–µ—Å—Ç—å" if __import__("os").environ.get("CLIENT_BOT_TOKEN") or __import__("os").environ.get("BOT_TOKEN") else "–ù–ï–¢"
    print(f"Telegram: –≤–æ—Ä–∫–µ—Ä—ã –Ω–∞ —Å–º–µ–Ω–µ (round-robin), token={_tok}", flush=True)
    print()

    init_db(args.db_path)

    round_num = 0
    while True:
        round_num += 1
        print(f"\n{'='*60}")
        print(f"–†–∞—É–Ω–¥ {round_num}")
        print(f"{'='*60}")

        workers_on_shift = get_workers_on_shift(args.db_path)
        if not workers_on_shift:
            print("‚è≠ –ù–µ—Ç –≤–æ—Ä–∫–µ—Ä–æ–≤ –Ω–∞ —Å–º–µ–Ω–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫ —Ä–∞—É–Ω–¥–∞ (–±–µ–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø—Ä–æ–∫—Å–∏)")
            if args.once:
                break
            pause_sec = args.pause_minutes * 60
            print(f"\n–ü–∞—É–∑–∞ {args.pause_minutes} –º–∏–Ω. –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏...")
            time.sleep(pause_sec)
            continue

        existing_item_ids = load_existing_item_ids(args.db_path)
        print(f"–í –ë–î: {len(existing_item_ids)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        print(f"–í–æ—Ä–∫–µ—Ä–æ–≤ –Ω–∞ —Å–º–µ–Ω–µ: {len(workers_on_shift)}")

        saved, total_sec, cat_times = asyncio.run(run_one_round_async(
            proxies, args.socks5, args.timeout, args.max_age, args.limit,
            args.db_path, existing_item_ids, args.concurrent,
        ))

        avg_s = total_sec / len(cat_times) if cat_times else 0
        print(f"\n–†–∞—É–Ω–¥ {round_num}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved} –Ω–æ–≤—ã—Ö | {total_sec:.1f} —Å –≤—Å–µ–≥–æ | "
              f"{avg_s:.1f} —Å/–∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ —Å—Ä–µ–¥–Ω–µ–º")

        if ENVIRONMENT == "dev" and TELEGRAM_CHAT_ID:
            send_round_summary(round_num, saved, total_sec, avg_s, len(existing_item_ids))

        if args.once:
            break

        pause_sec = args.pause_minutes * 60
        print(f"\n–ü–∞—É–∑–∞ {args.pause_minutes} –º–∏–Ω. –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞—É–Ω–¥–∞...")
        time.sleep(pause_sec)

    return 0


if __name__ == "__main__":
    sys.exit(main())
